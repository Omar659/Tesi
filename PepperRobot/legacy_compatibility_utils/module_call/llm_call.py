import openai
from openai import OpenAI
import json

class LlmCaller:
    def __init__(self):
        """
        Initializes the LlmCaller class by setting up the OpenAI client.
        - Loads API credentials from a JSON file and creates an OpenAI client instance.
        """
        # Initialize the OpenAI client using credentials loaded from a JSON file.
        self.client = self.__get_client()
        
    def __get_client(self):
        """
        Loads API credentials from a JSON file and creates an OpenAI client instance.

        Returns:
        - OpenAI client instance configured with the API key and base URL.
        """
        # Open and read the JSON file containing API credentials.
        with open("./token.json", 'r', encoding='utf-8') as file:
            data = json.load(file)

        # Create and return an OpenAI client instance with the API key and base URL.
        client = openai.OpenAI(
            api_key=data["api_key"],  # API key for authenticating requests.
            base_url=data["base_url"],  # Base URL for the OpenAI API.
        )
        return client
    
    def get_answer(self, system_prompt, user_prompt, model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo", max_tokens=512):
        """
        Generates a response from the language model based on the provided prompts.

        Parameters:
        - system_prompt (str): The system-level prompt to set the behavior of the model.
        - user_prompt (str): The user-level prompt or query for which the response is generated.
        - model (str): The model to use for generating responses (default is "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo").
        - max_tokens (int): The maximum number of tokens to generate in the response (default is 512).

        Returns:
        - str: The content of the response generated by the model.
        """
        # Prepare the messages in the required format for the model.
        messages = [
            {
                "role": "system",  # Role of the first message, indicating the system prompt.
                "content": system_prompt  # Content of the system prompt.
            },
            {
                "role": "user",  # Role of the second message, indicating the user prompt.
                "content": user_prompt,  # Content of the user prompt.
            }
        ]
        
        # Request a completion from the OpenAI model using the provided prompts and settings.
        code_completion = self.client.chat.completions.create(
            model=model,            # Specify the model to use for generating responses.
            messages=messages,      # Provide the formatted messages to the model.
            temperature=0.7,        # Controls the randomness of the output; higher values result in more diverse outputs.
            max_tokens=max_tokens,  # Limit the length of the generated response.
            stop=["<step>"],        # Specify a stopping sequence to terminate the response generation.
            frequency_penalty=1,    # Penalizes new tokens based on their frequency in the text; higher values discourage frequent tokens.
            presence_penalty=1,     # Penalizes new tokens based on their presence in the text; higher values encourage new topics.
            top_p=0.7               # Controls the diversity of the output by sampling from the top tokens with cumulative probability up to 0.7.
        )
        
        # Return the content of the generated response from the model. 
        return code_completion.choices[0].message.content