import openai
from openai import OpenAI
import json

class LlmCaller:
    def __init__(self):
        """
        Initializes the LlmCaller class by setting up the OpenAI client.
        - Loads API credentials from a JSON file and creates an OpenAI client instance.
        """
        # Initialize the OpenAI client using credentials loaded from a JSON file.
        self.client = self.__get_client()
        
    def __get_client(self):
        """
        Loads API credentials from a JSON file and creates an OpenAI client instance.

        Returns:
        - OpenAI client instance configured with the API key and base URL.
        """
        # Open and read the JSON file containing API credentials.
        with open("./token.json", 'r', encoding='utf-8') as file:
            data = json.load(file)

        # Create and return an OpenAI client instance with the API key and base URL.
        client = openai.OpenAI(
            api_key=data["api_key"],  # API key for authenticating requests.
            base_url=data["base_url"],  # Base URL for the OpenAI API.
        )
        return client
    
    def get_answer(self, system_prompt, user_prompt, model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo", max_tokens=512):
        """
        Generates a response from the language model based on the provided prompts.

        Parameters:
        - system_prompt (str): The system-level prompt to set the behavior of the model.
        - user_prompt (str): The user-level prompt or query for which the response is generated.
        - model (str): The model to use for generating responses (default is "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo").
        - max_tokens (int): The maximum number of tokens to generate in the response (default is 512).

        Returns:
        - str: The content of the response generated by the model.
        """
        # Prepare the messages in the required format for the model.
        messages = [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": user_prompt,
            }
        ]
        
        code_completion = self.client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.7,
            max_tokens=max_tokens,
            stop=[
                "<step>"
            ],
            frequency_penalty=1,
            presence_penalty=1,
            top_p=0.7
        )
        
        return code_completion.choices[0].message.content